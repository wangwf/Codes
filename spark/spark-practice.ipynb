{
 "metadata": {
  "name": "",
  "signature": "sha256:e52f1a90e16d69eb1358183bbcb2fa0942aa1fe8ee700710bc61e589607edbcf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#SPark\n",
      "To launch PySpark in Ipython, set the IPYTHON variable to 1 when running bin/pyspark\n",
      " $IPYTHON=1 ./bin/pyspark\n",
      "\n",
      " Alternatively, you can customize. to launch the IPython Notebook with Pylab\n",
      "\n",
      " $IPYTHON_OPTS=\"notebook --pylab inline\" ./bin/pyspark"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Functional\n",
      "* map\n",
      "* reduce\n",
      "* filter\n",
      "* lambda\n",
      "* itertools, pytoolz\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lambda_square = lambda x: x*x\n",
      "map(lambda_square, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# reduce apply a function with two arguments cumulatively to the container\n",
      "reduce(lambda x,y: x+y, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "45"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filter, constructs a new list for items where the applied function is true\n",
      "filter(lambda x: x%2==0, range(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 4,
       "text": [
        "[0, 2, 4, 6, 8]"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "from pyspark import SparkContext\n",
      "if 'sc' not in globals():\n",
      "    CLUSTER_URL= # \"spark://ec2-50-16-173-245.compute-1.amazonaws.com:7077\"\n",
      "    sc = SparkContext(CLUSTER_URL,\"example\")\n",
      "else:\n",
      "    print \"sc is here\"\n",
      "    '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 6,
       "text": [
        "'\\nfrom pyspark import SparkContext\\nif \\'sc\\' not in globals():\\n    CLUSTER_URL= # \"spark://ec2-50-16-173-245.compute-1.amazonaws.com:7077\"\\n    sc = SparkContext(CLUSTER_URL,\"example\")\\nelse:\\n    print \"sc is here\"\\n    '"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Spark \n",
      "\n",
      "Actions return values\n",
      "\n",
      "* collect,    Returns a list from all elements of an RDD\n",
      "* reduce\n",
      "* take\n",
      "* count\n",
      "\n",
      "Transformations returns pointers to new RDDs\n",
      "\n",
      "* map, flatmap\n",
      "* reduceByKey\n",
      "* filter\n",
      "* glom,     returns an RDD list form each partition of an RDD\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "rdd = sc.parallelize(np.arange(20), numSlices =5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "20"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for x in rdd.glom().collect():\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1, 2, 3]\n",
        "[4, 5, 6, 7]\n",
        "[8, 9, 10, 11]\n",
        "[12, 13, 14, 15]\n",
        "[16, 17, 18, 19]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize(np.arange(20), numSlices=10)\n",
      "for x in rdd.glom().collect():\n",
      "    print x"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[0, 1]\n",
        "[2, 3]\n",
        "[4, 5]\n",
        "[6, 7]\n",
        "[8, 9]\n",
        "[10, 11]\n",
        "[12, 13]\n",
        "[14, 15]\n",
        "[16, 17]\n",
        "[18, 19]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# map and Flatmap"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([ [2, 3, 4],[0, 1],[5, 6, 7, 8] ])\n",
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "[[2, 3, 4], [0, 1], [5, 6, 7, 8]]"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.map(lambda x: range(len(x))).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 25,
       "text": [
        "[[0, 1, 2], [0, 1], [0, 1, 2, 3]]"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.flatMap(lambda x: range(len(x))).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 26,
       "text": [
        "[0, 1, 2, 0, 1, 0, 1, 2, 3]"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# reduce\n",
      "* reduce()\n",
      "* reduceByKey()\n",
      "* countByKey()\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#reduce\n",
      "rdd.flatMap(lambda x:x).reduce(lambda x,y: x+y)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 27,
       "text": [
        "36"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
      "rdd.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 30,
       "text": [
        "[('a', 1), ('b', 1), ('a', 2)]"
       ]
      }
     ],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.reduceByKey(lambda x,y:x+y).collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 31,
       "text": [
        "[('a', 3), ('b', 1)]"
       ]
      }
     ],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rdd.countByKey()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "defaultdict(<type 'int'>, {'a': 2, 'b': 1})"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Computation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import os\n",
      "\n",
      "def work(x):\n",
      "    start_time = time.time()\n",
      "    time.sleep(x)\n",
      "    end_time = time.time()\n",
      "    return {'id':os.getpid(), 'start':start_time, 'end_time':end_time}\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "np.random.seed(1045)\n",
      "job_times = np.random.uniform(0.4, 0.6, 240)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'Estimated serial time = {0:0.2f}'.format(job_times.sum())\n",
      "print 'Amdahls parallel time = {0:0.2f}'.format(job_times.sum()/12.)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Estimated serial time = 119.80\n",
        "Amdahls parallel time = 9.98\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import matplotlib.pyplot as plt\n",
      "import pandas as pd\n",
      "\n",
      "def plot_workflow(results):\n",
      "    res = pd.DataFrame(results)\n",
      "    ids = list(set(res['id']))\n",
      "    id_dic = dict( [k,v+0.65] for k,v in zip(ids, range(len(ids))))\n",
      "    fig, ax = plt.subplots(figsize=(8, 6))\n",
      "\n",
      "    tmin = res['start'].min()    \n",
      "    for i in res.index:\n",
      "        x_start = res.ix[i]['start'] - tmin\n",
      "        x_end = res.ix[i]['end_time'] - tmin - x_start\n",
      "        x_id = id_dic[res.ix[i]['id']]\n",
      "        ax.add_patch(plt.Rectangle((x_start, x_id), \n",
      "                                   x_end, 0.8, \n",
      "                                   alpha=0.5, \n",
      "                                   color='grey'))\n",
      "    \n",
      "    ax.set_ylim(0.5, len(ids)+0.5)\n",
      "    ax.set_xlim(0, res['end_time'].max() - tmin)\n",
      "    ax.set_ylabel(\"Worker\")\n",
      "    ax.set_xlabel(\"seconds\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "jobs = sc.parallelize(job_times)\n",
      "print jobs.count()\n",
      "results = jobs.map(work)\n",
      "%time res = results.collect()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "240\n",
        "CPU times: user 9.53 ms, sys: 5.16 ms, total: 14.7 ms"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Wall time: 15.7 s\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plot_workflow(res)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAeUAAAF6CAYAAAAnAED0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlmfYRQtVAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b306f50>"
       ]
      }
     ],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "data = np.arange(1000).reshape(100,10)\n",
      "print data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(100, 10)\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "tmp = pd.DataFrame(data, \n",
      "             columns=['x{0}'.format(i) for i in range(data.shape[1])])\n",
      "tmp.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>x0</th>\n",
        "      <th>x1</th>\n",
        "      <th>x2</th>\n",
        "      <th>x3</th>\n",
        "      <th>x4</th>\n",
        "      <th>x5</th>\n",
        "      <th>x6</th>\n",
        "      <th>x7</th>\n",
        "      <th>x8</th>\n",
        "      <th>x9</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>  0</td>\n",
        "      <td>  1</td>\n",
        "      <td>  2</td>\n",
        "      <td>  3</td>\n",
        "      <td>  4</td>\n",
        "      <td>  5</td>\n",
        "      <td>  6</td>\n",
        "      <td>  7</td>\n",
        "      <td>  8</td>\n",
        "      <td>  9</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> 10</td>\n",
        "      <td> 11</td>\n",
        "      <td> 12</td>\n",
        "      <td> 13</td>\n",
        "      <td> 14</td>\n",
        "      <td> 15</td>\n",
        "      <td> 16</td>\n",
        "      <td> 17</td>\n",
        "      <td> 18</td>\n",
        "      <td> 19</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> 20</td>\n",
        "      <td> 21</td>\n",
        "      <td> 22</td>\n",
        "      <td> 23</td>\n",
        "      <td> 24</td>\n",
        "      <td> 25</td>\n",
        "      <td> 26</td>\n",
        "      <td> 27</td>\n",
        "      <td> 28</td>\n",
        "      <td> 29</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> 30</td>\n",
        "      <td> 31</td>\n",
        "      <td> 32</td>\n",
        "      <td> 33</td>\n",
        "      <td> 34</td>\n",
        "      <td> 35</td>\n",
        "      <td> 36</td>\n",
        "      <td> 37</td>\n",
        "      <td> 38</td>\n",
        "      <td> 39</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> 40</td>\n",
        "      <td> 41</td>\n",
        "      <td> 42</td>\n",
        "      <td> 43</td>\n",
        "      <td> 44</td>\n",
        "      <td> 45</td>\n",
        "      <td> 46</td>\n",
        "      <td> 47</td>\n",
        "      <td> 48</td>\n",
        "      <td> 49</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 17,
       "text": [
        "   x0  x1  x2  x3  x4  x5  x6  x7  x8  x9\n",
        "0   0   1   2   3   4   5   6   7   8   9\n",
        "1  10  11  12  13  14  15  16  17  18  19\n",
        "2  20  21  22  23  24  25  26  27  28  29\n",
        "3  30  31  32  33  34  35  36  37  38  39\n",
        "4  40  41  42  43  44  45  46  47  48  49"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp.sum(axis=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0       45\n",
        "1      145\n",
        "2      245\n",
        "3      345\n",
        "4      445\n",
        "5      545\n",
        "6      645\n",
        "7      745\n",
        "8      845\n",
        "9      945\n",
        "10    1045\n",
        "11    1145\n",
        "12    1245\n",
        "13    1345\n",
        "14    1445\n",
        "...\n",
        "85    8545\n",
        "86    8645\n",
        "87    8745\n",
        "88    8845\n",
        "89    8945\n",
        "90    9045\n",
        "91    9145\n",
        "92    9245\n",
        "93    9345\n",
        "94    9445\n",
        "95    9545\n",
        "96    9645\n",
        "97    9745\n",
        "98    9845\n",
        "99    9945\n",
        "Length: 100, dtype: int64"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp.to_csv(\"numbers.csv\",index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = sc.textFile('numbers.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines.take(2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "[u'x0,x1,x2,x3,x4,x5,x6,x7,x8,x9', u'0,1,2,3,4,5,6,7,8,9']"
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for l in lines.take(2):\n",
      "    print l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "x0,x1,x2,x3,x4,x5,x6,x7,x8,x9\n",
        "0,1,2,3,4,5,6,7,8,9\n"
       ]
      }
     ],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines.take(1)[0].find(\"x\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 37,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines = lines.filter(lambda x: x.find('x') !=0)\n",
      "for l in lines.take(2):\n",
      "    print l"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0,1,2,3,4,5,6,7,8,9\n",
        "10,11,12,13,14,15,16,17,18,19\n"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = lines.map(lambda x: x.split(\",\"))\n",
      "data.take(3),data.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "([[u'0', u'1', u'2', u'3', u'4', u'5', u'6', u'7', u'8', u'9'],\n",
        "  [u'10', u'11', u'12', u'13', u'14', u'15', u'16', u'17', u'18', u'19'],\n",
        "  [u'20', u'21', u'22', u'23', u'24', u'25', u'26', u'27', u'28', u'29']],\n",
        " 100)"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Row sum\n",
      "\n",
      "def row_sum(x):\n",
      "    int_x = map(lambda x: int(x), x)\n",
      "    return sum(int_x)\n",
      "\n",
      "data_row_sum = data.map(row_sum)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print data_row_sum.collect()\n",
      "print data_row_sum.count()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[45, 145, 245, 345, 445, 545, 645, 745, 845, 945, 1045, 1145, 1245, 1345, 1445, 1545, 1645, 1745, 1845, 1945, 2045, 2145, 2245, 2345, 2445, 2545, 2645, 2745, 2845, 2945, 3045, 3145, 3245, 3345, 3445, 3545, 3645, 3745, 3845, 3945, 4045, 4145, 4245, 4345, 4445, 4545, 4645, 4745, 4845, 4945, 5045, 5145, 5245, 5345, 5445, 5545, 5645, 5745, 5845, 5945, 6045, 6145, 6245, 6345, 6445, 6545, 6645, 6745, 6845, 6945, 7045, 7145, 7245, 7345, 7445, 7545, 7645, 7745, 7845, 7945, 8045, 8145, 8245, 8345, 8445, 8545, 8645, 8745, 8845, 8945, 9045, 9145, 9245, 9345, 9445, 9545, 9645, 9745, 9845, 9945]\n",
        "100"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Col Sum\n",
      "def col_key(x):\n",
      "    for i,value in enumerate(x):\n",
      "        yield (i, int(value))\n",
      "        \n",
      "tmp = data.flatMap(col_key)\n",
      "tmp.take(3)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 59,
       "text": [
        "[(0, 0), (1, 1), (2, 2)]"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tmp = tmp.groupByKey()\n",
      "'''\n",
      "for i in tmp.take(2):\n",
      "    print type(i),i[1]\n",
      "    for ii in i[1]:\n",
      "        print ii\n",
      "'''\n",
      "'''\n",
      "tmp = tmp.groupByByKey()\n",
      "for i in tmp.take(2):\n",
      "    print i\n",
      "    '''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 58,
       "text": [
        "'\\ntmp = tmp.groupByByKey()\\nfor i in tmp.take(2):\\n    print i\\n    '"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_col_sum = tmp.map(lambda x: sum(x[1]))\n",
      "for i in data_col_sum.take(2):\n",
      "    print i"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[<pyspark.resultiterable.ResultIterable object at 0x56a3050>]\n",
        "[<pyspark.resultiterable.ResultIterable object at 0x56a3350>]\n"
       ]
      }
     ],
     "prompt_number": 96
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 101
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "points = sc.textFile('iris_train.csv',10) #across 10 cpus\n",
      "points.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o500.splits.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/wangwf/work/Codes/spark/iris_train.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:175)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:50)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.java.JavaRDDLike$class.splits(JavaRDDLike.scala:47)\n\tat org.apache.spark.api.java.JavaRDD.splits(JavaRDD.scala:29)\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-60-9ea1dc83ca44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris_train.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#across 10 cpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;31m# we have per-split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m                 \u001b[0mpartitionsToTake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mpartitionsToTake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 537\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o500.splits.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/wangwf/work/Codes/spark/iris_train.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:175)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:50)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.java.JavaRDDLike$class.splits(JavaRDDLike.scala:47)\n\tat org.apache.spark.api.java.JavaRDD.splits(JavaRDD.scala:29)\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "\n",
      "#parsed_data = points.map(lambda line: np.array([float(x) for x in line.split(',')[:4]]))\n",
      "#parsed_data.take(5)\n",
      "#points.map(lambda line: x for x in line.split(','))\n",
      "points = points.filter(lambda x: x.find(\"sepal\")!=0)\\\n",
      "    .map(lambda line: line[:line.rfind(\",\")])\\\n",
      "    .map(lambda line:  np.array([float(x) for x in line.split(',')[:4]])) \n",
      "points.take(2)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'points' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-b1ba40f5d9bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#parsed_data.take(5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#points.map(lambda line: x for x in line.split(','))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mpoints\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sepal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpoints\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'points' is not defined"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parsed_data = points\n",
      "model = LogisticRegressionWithSGD.train(parsed_data)\n",
      "\n",
      "y = parsed_data.map(lambda x: int(x[0]))\n",
      "y.take(5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 0
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.classification import LogisticRegressionWithSGD\n",
      "from pyspark.mllib.regression import LabeledPoint\n",
      "from numpy import array\n",
      "\n",
      "# Load and parse the data\n",
      "def parsePoint(line):\n",
      "    values = [float(x) for x in line.split(' ')]\n",
      "    return LabeledPoint(values[0], values[1:])\n",
      "\n",
      "dataDir =\"/Users/work/venor/venor-research/spark-1.2.0/\"\n",
      "data = sc.textFile(dataDir+\"data/mllib/sample_svm_data.txt\")\n",
      "parsedData = data.map(parsePoint)\n",
      "\n",
      "parsedData.take(2)\n",
      "'''\n",
      "# Build the model\n",
      "model = LogisticRegressionWithSGD.train(parsedData)\n",
      "\n",
      "# Evaluating the model on training data\n",
      "labelsAndPreds = parsedData.map(lambda p: (p.label, model.predict(p.features)))\n",
      "trainErr = labelsAndPreds.filter(lambda (v, p): v != p).count() / float(parsedData.count())\n",
      "print(\"Training Error = \" + str(trainErr))\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Py4JJavaError",
       "evalue": "An error occurred while calling o33.splits.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/work/venor/venor-research/spark-1.2.0/data/mllib/sample_svm_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:175)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:50)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.java.JavaRDDLike$class.splits(JavaRDDLike.scala:47)\n\tat org.apache.spark.api.java.JavaRDD.splits(JavaRDD.scala:29)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-3-2ddc2b073cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mparsedData\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsePoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mparsedData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m '''\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m    863\u001b[0m         \u001b[0;31m# we have per-split.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_JavaStackTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mpartition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m                 \u001b[0mpartitionsToTake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m                 \u001b[0mpartitionsToTake\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpartition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         return_value = get_return_value(answer, self.gateway_client,\n\u001b[0;32m--> 537\u001b[0;31m                 self.target_id, self.name)\n\u001b[0m\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/wangwf/work/venor/venor-research/spark-1.0.1/spark/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    299\u001b[0m                     \u001b[0;34m'An error occurred while calling {0}{1}{2}.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     format(target_id, '.', name), value)\n\u001b[0m\u001b[1;32m    301\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                 raise Py4JError(\n",
        "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.splits.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/Users/work/venor/venor-research/spark-1.2.0/data/mllib/sample_svm_data.txt\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:197)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:175)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.python.PythonRDD.getPartitions(PythonRDD.scala:50)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:202)\n\tat scala.Option.getOrElse(Option.scala:120)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:202)\n\tat org.apache.spark.api.java.JavaRDDLike$class.splits(JavaRDDLike.scala:47)\n\tat org.apache.spark.api.java.JavaRDD.splits(JavaRDD.scala:29)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n\tat java.lang.reflect.Method.invoke(Method.java:597)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)\n\tat py4j.Gateway.invoke(Gateway.java:259)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:207)\n\tat java.lang.Thread.run(Thread.java:695)\n"
       ]
      }
     ],
     "prompt_number": 3
    }
   ],
   "metadata": {}
  }
 ]
}